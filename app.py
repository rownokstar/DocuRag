import os
import io
import logging
import tempfile
import streamlit as st
import pandas as pd
import fitz  # PyMuPDF
import camelot
import pytesseract
from PIL import Image
from typing import List, Tuple, Dict, Any

import google.generativeai as genai
# --- Ei line ta update kora hoyechilo ---
from langchain_core.documents import Document
# --- ---
from langchain_community.embeddings import HuggingFaceEmbeddings # Local embedding
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant # Vector store
# LangChain Community theke import kora hocche


# --- ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ‡¶è‡¶¨‡¶Ç ‡¶≤‡¶ó‡¶ø‡¶Ç ---

# ‡¶≤‡¶ó‡¶æ‡¶∞ ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (P: Clear logs for debugging)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Tesseract OCR-‡¶è‡¶∞ ‡¶™‡¶æ‡¶• (‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶ø ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ PATH-‡¶è ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá)
# ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶π‡¶≤‡ßá ‡¶è‡¶ü‡¶ø ‡¶Ü‡¶®‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Tesseract-‡¶è‡¶∞ ‡¶™‡¶æ‡¶• ‡¶¶‡¶ø‡¶®
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'


# --- ‡ßß. PDF ‡¶á‡¶®‡¶ú‡ßá‡¶∂‡¶® ‡¶è‡¶¨‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ---
# (I-1: PyMuPDF, Tesseract, Camelot)

def extract_text_with_ocr(page: fitz.Page, file_name: str, page_num: int) -> str:
    """‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç OCR ‡¶á‡¶Æ‡ßá‡¶ú ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§"""
    text = page.get_text("text")
    image_text = ""

    # ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø OCR ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã (P: Handle missing or scanned text)
    try:
        if len(text.strip()) < 100:  # ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶Ø‡¶º ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶Æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá OCR ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
            images = page.get_images(full=True)
            if images:
                logger.info(f"Running OCR on {file_name} page {page_num}...")
                for img_index, img in enumerate(images):
                    xref = img[0]
                    base_image = page.parent.extract_image(xref)
                    image_bytes = base_image["image"]
                    pil_image = Image.open(io.BytesIO(image_bytes))

                    # Tesseract ‡¶¶‡¶ø‡ßü‡ßá OCR
                    image_text += pytesseract.image_to_string(pil_image)
                    logger.info(f"OCR extracted {len(image_text)} chars from image {img_index} on page {page_num}.")
    except Exception as e:
        logger.warning(f"Could not run OCR on {file_name} page {page_num}: {e}")

    return text + "\n" + image_text

def extract_tables_with_camelot(temp_pdf_path: str, file_name: str, page_num: int) -> str:
    """Camelot ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§"""
    table_content = ""
    try:
        # Camelot ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡¶æ‡¶• ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡¶°‡¶º‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá, ‡¶¨‡¶æ‡¶á‡¶ü ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡ßÄ‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶Ø‡¶º
        tables = camelot.read_pdf(temp_pdf_path, pages=str(page_num), flavor='lattice')

        if tables.n > 0:
            logger.info(f"Extracted {tables.n} tables from {file_name} page {page_num}.")
            for i, table in enumerate(tables):
                table_df = table.df
                table_content += f"\n--- Table {i+1} on Page {page_num} ---\n"
                table_content += table_df.to_markdown(index=False)
                table_content += f"\n--- End of Table {i+1} ---\n"

                # ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ (E-3: Table Extraction)
                if 'tables' not in st.session_state:
                    st.session_state.tables = {}
                table_key = f"{file_name}_p{page_num}_t{i+1}"
                st.session_state.tables[table_key] = table_df

    except Exception as e:
        # (P: Table extraction failure fallback)
        logger.warning(f"Camelot failed to extract tables from {file_name} page {page_num}: {e}")

    return table_content

def process_uploaded_pdf(uploaded_file: Any) -> List[Tuple[int, str, str]]:
    """
    ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßá‡•§
    PyMuPDF, Tesseract, ‡¶è‡¶¨‡¶Ç Camelot ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶á‡¶Æ‡ßá‡¶ú (OCR), ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§
    """
    file_name = uploaded_file.name
    logger.info(f"Starting processing for: {file_name}")

    # Camelot-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶´‡¶æ‡¶á‡¶≤‡ßá PDF ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡¶æ
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(uploaded_file.getvalue())
        temp_pdf_path = temp_file.name

    all_page_data = []

    try:
        # PyMuPDF (fitz) ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá PDF ‡¶ñ‡ßã‡¶≤‡¶æ
        doc = fitz.open(stream=uploaded_file.getvalue(), filetype="pdf")

        for page_num_zero_based in range(len(doc)):
            page_num_one_based = page_num_zero_based + 1
            page = doc.load_page(page_num_zero_based)

            # ‡ßß. ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç OCR ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶®
            page_text = extract_text_with_ocr(page, file_name, page_num_one_based)
            if page_text.strip():
                all_page_data.append((page_num_one_based, page_text, "text"))

            # ‡ß®. ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® (Camelot)
            table_text = extract_tables_with_camelot(temp_pdf_path, file_name, page_num_one_based)
            if table_text.strip():
                all_page_data.append((page_num_one_based, table_text, "table"))

        doc.close()

    except Exception as e:
        logger.error(f"Failed to process PDF {file_name}: {e}")
        st.error(f"Error processing {file_name}. Is it a valid PDF?")
    finally:
        # ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶´‡ßá‡¶≤‡¶æ
        os.remove(temp_pdf_path)
        logger.info(f"Finished processing for: {file_name}")

    return all_page_data


# --- ‡ß®. ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï‡¶ø‡¶Ç ---
# (I-2: Chunking, Cleaning, Metadata)

def chunk_data(file_name: str, processed_data: List[Tuple[int, str, str]]) -> List[Document]:
    """‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶π LangChain Document ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,  # (I-2: ~500-800 tokens)
        chunk_overlap=100,
        length_function=len,
        add_start_index=True,
    )

    all_chunks = []
    for page_num, content, content_type in processed_data:
        # (I-2: Clean and normalize)
        cleaned_content = ' '.join(content.split())

        # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø
        metadata = {
            "source": file_name,
            "page": page_num,
            "type": content_type
        }

        # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ
        chunks = text_splitter.split_text(cleaned_content)

        for chunk in chunks:
            all_chunks.append(Document(page_content=chunk, metadata=metadata))

    logger.info(f"Created {len(all_chunks)} chunks for {file_name}.")
    return all_chunks


# --- ‡ß©. ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ---
# (I-3: Local Embeddings, Qdrant)

@st.cache_resource(ttl=3600)
def get_embeddings_model():
    """‡¶´‡ßç‡¶∞‡¶ø, ‡¶ì‡¶™‡ßá‡¶®-‡¶∏‡ßã‡¶∞‡ßç‡¶∏ HuggingFace ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡•§"""
    try:
        logger.info("Loading local embeddings model (all-MiniLM-L6-v2)...")
        # ‡¶è‡¶á ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø Streamlit-‡¶è‡¶∞ CPU-‡¶§‡ßá ‡¶ö‡¶≤‡¶¨‡ßá
        model_name = "sentence-transformers/all-MiniLM-L6-v2"
        model_kwargs = {'device': 'cpu'}
        encode_kwargs = {'normalize_embeddings': False}

        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        logger.info("Local embeddings model loaded successfully.")
        return embeddings
    except Exception as e:
        logger.error(f"Failed to initialize local embeddings model: {e}")
        st.error(f"Error initializing local embeddings model. Error: {e}")
        return None

# --- ‡¶è‡¶á ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ---
def create_vector_store(all_chunks: List[Document], embeddings_model: Any) -> Qdrant:
    """‡¶ö‡¶æ‡¶ô‡ßç‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø Qdrant ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§"""
    if not all_chunks:
        logger.warning("No chunks to process. Returning empty vector store.")
        return None

    logger.info(f"Creating Qdrant index from {len(all_chunks)} chunks...")

    try:
        # from_documents ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá, ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞‡ßá‡•§
        vector_store = Qdrant.from_documents(
            all_chunks,
            embeddings_model,
            location=":memory:",  # ‡¶á‡¶®-‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
            collection_name="docurag-collection"
        )

        logger.info("Qdrant index created successfully.")
        return vector_store

    except Exception as e:
        logger.error(f"Qdrant index creation failed: {e}")
        st.error(f"Failed to create vector store. Error: {e}")
        return None


# --- ‡ß™. RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
# (I-KA-4: RAG Pipeline, Gemini API, Citations)

def get_rag_response(query: str, vector_store: Any, genai_model: Any) -> Dict[str, Any]:
    """
    RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ï‡¶ø‡¶â‡¶ü ‡¶ï‡¶∞‡ßá: ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠, ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶§‡ßà‡¶∞‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü‡•§
    """
    logger.info(f"Received query: {query}")

    # ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶° ‡¶ö‡ßá‡¶ï (E-3 / E-4)
    if query.strip().lower().startswith("show table"):
        parts = query.split()
        if len(parts) >= 3:
            table_key = parts[2]
            if 'tables' not in st.session_state or table_key not in st.session_state.tables:
                return {"answer": f"Sorry, table '{table_key}' not found in processed documents."}
            else:
                return {"answer": f"Displaying table: {table_key}", "table_data": st.session_state.tables[table_key]}
        else:
             return {"answer": "Please specify the table key. You can find keys in the 'Processed Tables' section."}

    # ‡ßß. ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠ (Retrieve)
    retriever = vector_store.as_retriever(search_k=4) # (I-4: top-k (3-5))
    retrieved_docs = retriever.invoke(query) # Modern Langchain uses invoke

    if not retrieved_docs:
        logger.warning("No relevant documents found.")
        return {"answer": "I'm sorry, I couldn't find any relevant information in the uploaded documents to answer your question."}

    # ‡ß®. ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶§‡ßà‡¶∞‡¶ø (Augment)
    context = ""
    sources = set()
    for doc in retrieved_docs:
        context += f"\n--- Context from {doc.metadata['source']}, Page {doc.metadata['page']} ---\n"
        context += doc.page_content
        context += f"\n--- End of Context ---\n"
        sources.add(f"{doc.metadata['source']}, Page {doc.metadata['page']}")

    citation_str = " (Source: " + ", ".join(sorted(list(sources))) + ")"

    system_prompt = f"""
    You are DocuRAG, a specialized AI assistant for document analysis.
    Your task is to answer the user's question based *only* on the provided context.
    Do not use any external knowledge.
    If the answer is not found in the context, state that clearly.
    Be concise and professional.

    CONTEXT:
    {context}

    QUESTION:
    {query}

    ANSWER (based *only* on the context):
    """

    # ‡ß©. ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü (Generate)
    logger.info("Sending prompt to Gemini-Pro...")
    try:
        response = genai_model.generate_content(system_prompt)
        # Check if response has parts and text (robustness)
        if hasattr(response, 'text'):
             final_answer = response.text + citation_str
        elif hasattr(response, 'parts') and response.parts:
             final_answer = response.parts[0].text + citation_str
        else:
             # Handle cases where response might be empty or structured differently
             logger.warning("Received an unexpected response structure from Gemini API.")
             final_answer = "Sorry, I could not generate a response based on the context." + citation_str

        return {"answer": final_answer}
    except Exception as e:
        logger.error(f"Gemini API call failed: {e}")
        return {"answer": f"Error generating response from AI model. Please check logs. Error: {e}"}


# --- ‡ß´. ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ (Streamlit UI) ---
# (I-5: Streamlit UI)

def main():
    st.set_page_config(page_title="DocuRAG", page_icon="üìÑ", layout="wide")
    st.title("üìÑ DocuRAG: AI Document Intelligence PoC")

    # --- ‡¶∏‡¶æ‡¶á‡¶°‡¶¨‡¶æ‡¶∞: API ‡¶ï‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ---
    with st.sidebar:
        st.header("Configuration")

        # Google API ‡¶ï‡ßÄ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü (‡¶è‡¶ü‡¶ø ‡¶è‡¶ñ‡¶®‡ßã ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®)
        api_key = st.text_input("Enter your Google API Key", type="password")

        if api_key:
            try:
                genai.configure(api_key=api_key)
                st.session_state.api_key_configured = True
                logger.info("Google API Key configured.")
            except Exception as e:
                st.error(f"Failed to configure API key: {e}")
                st.session_state.api_key_configured = False
        else:
            st.session_state.api_key_configured = False
            st.warning("Please enter your Google API Key to proceed.")

        st.divider()

        # ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡¶æ‡¶∞
        uploaded_files = st.file_uploader(
            "Upload your PDF documents",
            type="pdf",
            accept_multiple_files=True,
            disabled=not st.session_state.api_key_configured
        )

        # --- ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶¨‡¶æ‡¶ü‡¶® (‡¶è‡¶á ‡¶≤‡¶ú‡¶ø‡¶ï‡¶ü‡¶ø AttributeError-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá) ---
        if st.button("Process Documents", disabled=not uploaded_files or not st.session_state.api_key_configured):
            if uploaded_files:
                # Reset previous state if reprocessing
                st.session_state.documents_processed = False
                st.session_state.vector_store = None
                st.session_state.messages = [{"role": "assistant", "content": "Processing documents..."}] # Clear chat history on reprocess
                st.rerun() # Force UI update

                with st.spinner("Processing documents... (Loading local model first, this may take a moment)"):
                    all_chunks = []

                    # ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ (‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø API Key ‡¶≤‡¶æ‡¶ó‡ßá ‡¶®‡¶æ)
                    embeddings_model = get_embeddings_model()

                    if embeddings_model:
                        for file in uploaded_files:
                            st.info(f"Ingesting '{file.name}'...")
                            # ‡ßß. ‡¶á‡¶®‡¶ú‡ßá‡¶∂‡¶® (‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶ì‡¶∏‡¶ø‡¶Ü‡¶∞, ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤)
                            page_data = process_uploaded_pdf(file)
                            # ‡ß®. ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï‡¶ø‡¶Ç
                            chunks = chunk_data(file.name, page_data)
                            all_chunks.extend(chunks)

                        # ‡ß©. ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø (‡¶∏‡¶†‡¶ø‡¶ï ‡¶≤‡¶ú‡¶ø‡¶ï ‡¶∏‡¶π)
                        if all_chunks:
                            # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶≠‡ßá‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶¨‡¶≤‡ßá ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
                            vector_store = create_vector_store(all_chunks, embeddings_model)

                            if vector_store:
                                # ‡¶∏‡¶´‡¶≤ ‡¶π‡¶≤‡ßá‡¶á ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
                                st.session_state.vector_store = vector_store
                                st.session_state.documents_processed = True
                                # Update initial message after processing
                                st.session_state.messages = [{"role": "assistant", "content": f"Processed {len(uploaded_files)} documents ({len(all_chunks)} chunks). Ready to chat!"}]
                                st.success("Processing complete. Ready to chat!") # Give success feedback
                                st.rerun() # Force UI update to show success and clear spinner
                            else:
                                # ‡¶Ø‡¶¶‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶®‡¶æ ‡¶π‡ßü
                                st.error("Failed to create vector store. Please check console logs.")
                                st.session_state.documents_processed = False
                        else:
                            st.warning("No text or table data could be extracted from the documents.")
                            st.session_state.documents_processed = False
                    else:
                        st.error("Cannot process documents: Local Embeddings model failed to load.")
                        st.session_state.documents_processed = False

        st.divider()

        # ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤‡ßá‡¶∞ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ (E-3)
        if 'tables' in st.session_state and st.session_state.tables:
            with st.expander("View Processed Tables"):
                st.info("Use 'show table <key>' to display a table in the chat.")
                for key in st.session_state.tables.keys():
                    st.code(key)

    # --- ‡¶Æ‡ßá‡¶á‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ ---

    # ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Welcome to DocuRAG! Please upload your documents and I'll help you analyze them."}]

    if "documents_processed" not in st.session_state:
        st.session_state.documents_processed = False

    if "tables" not in st.session_state:
        st.session_state.tables = {}

    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "table_data" in message:
                st.dataframe(message["table_data"]) # (E-3: Display table)
                # (E-4: Chart Visualization)
                try:
                    # ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶Ç ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ
                    df_numeric = message["table_data"].apply(pd.to_numeric, errors='coerce').dropna(axis=1)
                    if not df_numeric.empty and len(df_numeric.columns) > 1:
                        st.bar_chart(df_numeric, x=df_numeric.columns[0], y=df_numeric.columns[1])
                except Exception:
                    pass # ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶®‡¶æ ‡¶ó‡ßá‡¶≤‡ßá ‡¶∏‡¶æ‡¶á‡¶≤‡ßá‡¶®‡ßç‡¶ü‡¶≤‡¶ø ‡¶´‡ßá‡¶á‡¶≤ ‡¶π‡¶¨‡ßá

    # ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤‡¶ø‡¶Ç
    if prompt := st.chat_input("Ask a question about your documents..."):
        if not st.session_state.api_key_configured:
            st.info("Please configure your Google API Key in the sidebar first.")
        elif not st.session_state.documents_processed or not st.session_state.get("vector_store"):
             st.info("Please upload and process your documents first, or wait for processing to complete.")
        else:
            # ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶ü ‡¶∞‡ßá‡¶∏‡¶™‡¶®‡ßç‡¶∏ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü
            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):

                    # 'gemini-1.0-pro' ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
                    genai_model = genai.GenerativeModel('gemini-1.0-pro')

                    response_dict = get_rag_response(
                        prompt,
                        st.session_state.vector_store,
                        genai_model
                    )

                    st.markdown(response_dict["answer"])

                    # ‡¶∞‡ßá‡¶∏‡¶™‡¶®‡ßç‡¶∏ ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
                    assistant_message = {"role": "assistant", "content": response_dict["answer"]}
                    if "table_data" in response_dict:
                        assistant_message["table_data"] = response_dict["table_data"]
                        st.dataframe(response_dict["table_data"])

                    st.session_state.messages.append(assistant_message)


if __name__ == "__main__":
    main()
