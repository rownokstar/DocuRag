import os
import io
import logging
import tempfile
import streamlit as st
import pandas as pd
import fitz  # PyMuPDF
import camelot
import pytesseract
from PIL import Image
from typing import List, Tuple, Dict, Any

import google.generativeai as genai
from langchain.docstore.document import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_qdrant import Qdrant # ‡¶Ü‡¶Æ‡¶∞‡¶æ Qdrant ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø
from qdrant_client import QdrantClient


# --- ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ‡¶è‡¶¨‡¶Ç ‡¶≤‡¶ó‡¶ø‡¶Ç ---

# ‡¶≤‡¶ó‡¶æ‡¶∞ ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (P: Clear logs for debugging)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Tesseract OCR-‡¶è‡¶∞ ‡¶™‡¶æ‡¶• (‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶ø ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ PATH-‡¶è ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá)
# ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶π‡¶≤‡ßá ‡¶è‡¶ü‡¶ø ‡¶Ü‡¶®‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Tesseract-‡¶è‡¶∞ ‡¶™‡¶æ‡¶• ‡¶¶‡¶ø‡¶®
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'


# --- ‡ßß. PDF ‡¶á‡¶®‡¶ú‡ßá‡¶∂‡¶® ‡¶è‡¶¨‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ---
# (I-1: PyMuPDF, Tesseract, Camelot)

def extract_text_with_ocr(page: fitz.Page, file_name: str, page_num: int) -> str:
    """‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç OCR ‡¶á‡¶Æ‡ßá‡¶ú ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§"""
    text = page.get_text("text")
    image_text = ""
    
    # ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø OCR ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã (P: Handle missing or scanned text)
    try:
        if len(text.strip()) < 100:  # ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶Ø‡¶º ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶Æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá OCR ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
            images = page.get_images(full=True)
            if images:
                logger.info(f"Running OCR on {file_name} page {page_num}...")
                for img_index, img in enumerate(images):
                    xref = img[0]
                    base_image = page.parent.extract_image(xref)
                    image_bytes = base_image["image"]
                    pil_image = Image.open(io.BytesIO(image_bytes))
                    
                    # Tesseract ‡¶¶‡¶ø‡ßü‡ßá OCR
                    image_text += pytesseract.image_to_string(pil_image)
                    logger.info(f"OCR extracted {len(image_text)} chars from image {img_index} on page {page_num}.")
    except Exception as e:
        logger.warning(f"Could not run OCR on {file_name} page {page_num}: {e}")

    return text + "\n" + image_text

def extract_tables_with_camelot(temp_pdf_path: str, file_name: str, page_num: int) -> str:
    """Camelot ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§"""
    table_content = ""
    try:
        # Camelot ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡¶æ‡¶• ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡¶°‡¶º‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá, ‡¶¨‡¶æ‡¶á‡¶ü ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡ßÄ‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶Ø‡¶º
        tables = camelot.read_pdf(temp_pdf_path, pages=str(page_num), flavor='lattice')
        
        if tables.n > 0:
            logger.info(f"Extracted {tables.n} tables from {file_name} page {page_num}.")
            for i, table in enumerate(tables):
                table_df = table.df
                table_content += f"\n--- Table {i+1} on Page {page_num} ---\n"
                table_content += table_df.to_markdown(index=False)
                table_content += f"\n--- End of Table {i+1} ---\n"
                
                # ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ (E-3: Table Extraction)
                if 'tables' not in st.session_state:
                    st.session_state.tables = {}
                table_key = f"{file_name}_p{page_num}_t{i+1}"
                st.session_state.tables[table_key] = table_df
                
    except Exception as e:
        # (P: Table extraction failure fallback)
        logger.warning(f"Camelot failed to extract tables from {file_name} page {page_num}: {e}")
        
    return table_content

def process_uploaded_pdf(uploaded_file: Any) -> List[Tuple[int, str, str]]:
    """
    ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßá‡•§
    PyMuPDF, Tesseract, ‡¶è‡¶¨‡¶Ç Camelot ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶á‡¶Æ‡ßá‡¶ú (OCR), ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§
    """
    file_name = uploaded_file.name
    logger.info(f"Starting processing for: {file_name}")
    
    # Camelot-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶´‡¶æ‡¶á‡¶≤‡ßá PDF ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡¶æ
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(uploaded_file.getvalue())
        temp_pdf_path = temp_file.name

    all_page_data = []
    
    try:
        # PyMuPDF (fitz) ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá PDF ‡¶ñ‡ßã‡¶≤‡¶æ
        doc = fitz.open(stream=uploaded_file.getvalue(), filetype="pdf")

        for page_num_zero_based in range(len(doc)):
            page_num_one_based = page_num_zero_based + 1
            page = doc.load_page(page_num_zero_based)
            
            # ‡ßß. ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç OCR ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶®
            page_text = extract_text_with_ocr(page, file_name, page_num_one_based)
            if page_text.strip():
                all_page_data.append((page_num_one_based, page_text, "text"))
            
            # ‡ß®. ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® (Camelot)
            table_text = extract_tables_with_camelot(temp_pdf_path, file_name, page_num_one_based)
            if table_text.strip():
                all_page_data.append((page_num_one_based, table_text, "table"))

        doc.close()
        
    except Exception as e:
        logger.error(f"Failed to process PDF {file_name}: {e}")
        st.error(f"Error processing {file_name}. Is it a valid PDF?")
    finally:
        # ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶´‡ßá‡¶≤‡¶æ
        os.remove(temp_pdf_path)
        logger.info(f"Finished processing for: {file_name}")

    return all_page_data


# --- ‡ß®. ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï‡¶ø‡¶Ç ---
# (I-2: Chunking, Cleaning, Metadata)

def chunk_data(file_name: str, processed_data: List[Tuple[int, str, str]]) -> List[Document]:
    """‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶π LangChain Document ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,  # (I-2: ~500-800 tokens)
        chunk_overlap=100,
        length_function=len,
        add_start_index=True,
    )
    
    all_chunks = []
    for page_num, content, content_type in processed_data:
        # (I-2: Clean and normalize)
        cleaned_content = ' '.join(content.split())
        
        # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø
        metadata = {
            "source": file_name,
            "page": page_num,
            "type": content_type
        }
        
        # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ
        chunks = text_splitter.split_text(cleaned_content)
        
        for chunk in chunks:
            all_chunks.append(Document(page_content=chunk, metadata=metadata))
            
    logger.info(f"Created {len(all_chunks)} chunks for {file_name}.")
    return all_chunks


# --- ‡ß©. ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ---
# (I-3: gemini-embedding-001, Qdrant)

# --- +++ ‡¶è‡¶á ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø 503 ‡¶è‡¶∞‡¶∞ ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá +++ ---
@st.cache_resource(ttl=3600)
def get_embeddings_model(_api_key: str):
    """Google Generative AI ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶ï‡¶∞‡ßá‡•§"""
    try:
        # --- +++ ‡¶è‡¶á‡¶ñ‡¶æ‡¶®‡ßá ‡¶Æ‡ßÇ‡¶≤ ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶® +++ ---
        # ‡¶Ü‡¶Æ‡¶∞‡¶æ GoogleGenerativeAIEmbeddings-‡¶ï‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø API Key ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø
        return GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=_api_key  # ‡¶ï‡ßÄ-‡¶ü‡¶ø ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá
        )
    except Exception as e:
        logger.error(f"Failed to initialize embeddings model: {e}")
        st.error(f"Error initializing embeddings. Is your API key correct? Error: {e}")
        return None

# --- ‡¶è‡¶á ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø 504 ‡¶è‡¶∞‡¶∞ ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡¶ø‡¶Ç ‡¶∏‡¶π ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá ---
def create_vector_store(all_chunks: List[Document], embeddings_model: Any) -> Qdrant:
    """‡¶ö‡¶æ‡¶ô‡ßç‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø Qdrant ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§ (‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶∏‡¶π)"""
    if not all_chunks:
        logger.warning("No chunks to process. Returning empty vector store.")
        return None
        
    logger.info(f"Creating Qdrant index from {len(all_chunks)} chunks...")
    
    # ‡¶á‡¶®-‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø Qdrant ‡¶ï‡ßç‡¶≤‡¶æ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
    client = QdrantClient(location=":memory:")
    collection_name = "docurag-collection"
    
    try:
        # ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        vector_store = Qdrant(
            client=client,
            collection_name=collection_name,
            embeddings=embeddings_model
        )
        
        # ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö ‡¶∏‡¶æ‡¶á‡¶ú (Batch Size) ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶®
        batch_size = 100 # ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞‡ßá ‡ßß‡ß¶‡ß¶‡¶ü‡¶ø ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶¨‡ßá
        
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i + batch_size]
            logger.info(f"Adding batch {i//batch_size + 1}/{len(all_chunks)//batch_size + 1} with {len(batch)} chunks...")
            vector_store.add_documents(batch) # ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
            
        logger.info("Qdrant index created successfully with all batches.")
        return vector_store
        
    except Exception as e:
        # ‡¶è‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º‡¶∂‡¶á ‡¶≠‡ßÅ‡¶≤ API ‡¶ï‡ßÄ-‡¶è‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶ò‡¶ü‡ßá (‡¶Ö‡¶•‡¶¨‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡¶ø‡¶Ç ‡¶´‡ßá‡¶á‡¶≤ ‡¶π‡¶≤‡ßá)
        logger.error(f"Qdrant index creation/batching failed: {e}")
        st.error(f"Failed to create vector store. Is your API key correct? Error: {e}")
        return None


# --- ‡ß™. RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
# (I-KA-4: RAG Pipeline, Gemini API, Citations)

def get_rag_response(query: str, vector_store: Any, genai_model: Any) -> Dict[str, Any]:
    """
    RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ï‡¶ø‡¶â‡¶ü ‡¶ï‡¶∞‡ßá: ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠, ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶§‡ßà‡¶∞‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü‡•§
    """
    logger.info(f"Received query: {query}")
    
    # ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶° ‡¶ö‡ßá‡¶ï (E-3 / E-4)
    if query.strip().lower().startswith("show table"):
        parts = query.split()
        if len(parts) >= 3:
            table_key = parts[2]
            if 'tables' in st.session_state and table_key in st.session_state.tables:
                return {"answer": f"Displaying table: {table_key}", "table_data": st.session_state.tables[table_key]}
            else:
                return {"answer": f"Sorry, table '{table_key}' not found in processed documents."}
        else:
             return {"answer": "Please specify the table key. You can find keys in the 'Processed Tables' section."}

    # ‡ßß. ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠ (Retrieve)
    retriever = vector_store.as_retriever(search_k=4) # (I-4: top-k (3-5))
    retrieved_docs = retriever.get_relevant_documents(query)
    
    if not retrieved_docs:
        logger.warning("No relevant documents found.")
        return {"answer": "I'm sorry, I couldn't find any relevant information in the uploaded documents to answer your question."}

    # ‡ß®. ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶§‡ßà‡¶∞‡¶ø (Augment)
    context = ""
    sources = set()
    for doc in retrieved_docs:
        context += f"\n--- Context from {doc.metadata['source']}, Page {doc.metadata['page']} ---\n"
        context += doc.page_content
        context += f"\n--- End of Context ---\n"
        sources.add(f"{doc.metadata['source']}, Page {doc.metadata['page']}")
        
    citation_str = " (Source: " + ", ".join(sorted(list(sources))) + ")"
    
    system_prompt = f"""
    You are DocuRAG, a specialized AI assistant for document analysis.
    Your task is to answer the user's question based *only* on the provided context.
    Do not use any external knowledge.
    If the answer is not found in the context, state that clearly.
    Be concise and professional.
    
    CONTEXT:
    {context}
    
    QUESTION:
    {query}
    
    ANSWER (based *only* on the context):
    """
    
    # ‡ß©. ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü (Generate)
    logger.info("Sending prompt to Gemini-Pro...")
    try:
        response = genai_model.generate_content(system_prompt)
        final_answer = response.text + citation_str # (S: Output: Each answer must include citation)
        return {"answer": final_answer}
    except Exception as e:
        logger.error(f"Gemini API call failed: {e}")
        return {"answer": f"Error generating response from AI model. Please check logs. Error: {e}"}


# --- ‡ß´. ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ (Streamlit UI) ---
# (I-5: Streamlit UI)

def main():
    st.set_page_config(page_title="DocuRAG", page_icon="üìÑ", layout="wide")
    st.title("üìÑ DocuRAG: AI Document Intelligence PoC")
    
    # --- ‡¶∏‡¶æ‡¶á‡¶°‡¶¨‡¶æ‡¶∞: API ‡¶ï‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ---
    with st.sidebar:
        st.header("Configuration")
        
        # Google API ‡¶ï‡ßÄ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü
        api_key = st.text_input("Enter your Google API Key", type="password")
        
        if api_key:
            try:
                genai.configure(api_key=api_key)
                st.session_state.api_key_configured = True
                logger.info("Google API Key configured.")
            except Exception as e:
                st.error(f"Failed to configure API key: {e}")
                st.session_state.api_key_configured = False
        else:
            st.session_state.api_key_configured = False
            st.warning("Please enter your Google API Key to proceed.")

        st.divider()
        
        # ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡¶æ‡¶∞
        uploaded_files = st.file_uploader(
            "Upload your PDF documents",
            type="pdf",
            accept_multiple_files=True,
            disabled=not st.session_state.api_key_configured
        )
        
        # --- ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶¨‡¶æ‡¶ü‡¶® (‡¶è‡¶á ‡¶≤‡¶ú‡¶ø‡¶ï‡¶ü‡¶ø AttributeError-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá) ---
        if st.button("Process Documents", disabled=not uploaded_files or not st.session_state.api_key_configured):
            if uploaded_files:
                with st.spinner("Processing documents... This may take a few minutes for large files or OCR."):
                    all_chunks = []
                    
                    # ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ
                    # ‡¶è‡¶á‡¶ñ‡¶æ‡¶®‡ßá api_key ‡¶™‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá
                    embeddings_model = get_embeddings_model(api_key)
                    
                    if embeddings_model:
                        for file in uploaded_files:
                            st.info(f"Ingesting '{file.name}'...")
                            # ‡ßß. ‡¶á‡¶®‡¶ú‡ßá‡¶∂‡¶® (‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶ì‡¶∏‡¶ø‡¶Ü‡¶∞, ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤)
                            page_data = process_uploaded_pdf(file)
                            # ‡ß®. ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï‡¶ø‡¶Ç
                            chunks = chunk_data(file.name, page_data)
                            all_chunks.extend(chunks)
                        
                        # ‡ß©. ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø (‡¶∏‡¶†‡¶ø‡¶ï ‡¶≤‡¶ú‡¶ø‡¶ï ‡¶∏‡¶π)
                        if all_chunks:
                            # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶≠‡ßá‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶¨‡¶≤‡ßá ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
                            vector_store = create_vector_store(all_chunks, embeddings_model)
                            
                            if vector_store:
                                # ‡¶∏‡¶´‡¶≤ ‡¶π‡¶≤‡ßá‡¶á ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
                                st.session_state.vector_store = vector_store
                                st.session_state.documents_processed = True
                                st.success(f"Processed {len(uploaded_files)} documents ({len(all_chunks)} chunks). Ready to chat!")
                            else:
                                # ‡¶Ø‡¶¶‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶®‡¶æ ‡¶π‡ßü (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶≠‡ßÅ‡¶≤ API ‡¶ï‡ßÄ ‡¶¨‡¶æ 503/504 ‡¶è‡¶∞‡¶∞)
                                st.error("Failed to create vector store. Please check your API Key or console logs.")
                                st.session_state.documents_processed = False 
                        else:
                            st.warning("No text or table data could be extracted from the documents.")
                            st.session_state.documents_processed = False 
                    else:
                        st.error("Cannot process documents: Embeddings model failed to load.")
                        st.session_state.documents_processed = False 

        st.divider()
        
        # ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤‡ßá‡¶∞ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ (E-3)
        if 'tables' in st.session_state and st.session_state.tables:
            with st.expander("View Processed Tables"):
                st.info("Use 'show table <key>' to display a table in the chat.")
                for key in st.session_state.tables.keys():
                    st.code(key)

    # --- ‡¶Æ‡ßá‡¶á‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ ---
    
    # ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Welcome to DocuRAG! Please upload your documents and I'll help you analyze them."}]
    
    if "documents_processed" not in st.session_state:
        st.session_state.documents_processed = False
        
    if "tables" not in st.session_state:
        st.session_state.tables = {}

    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "table_data" in message:
                st.dataframe(message["table_data"]) # (E-3: Display table)
                # (E-4: Chart Visualization)
                try:
                    # ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶Ç ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ
                    df_numeric = message["table_data"].apply(pd.to_numeric, errors='coerce').dropna(axis=1)
                    if not df_numeric.empty and len(df_numeric.columns) > 1:
                        st.bar_chart(df_numeric, x=df_numeric.columns[0], y=df_numeric.columns[1])
                except Exception:
                    pass # ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶®‡¶æ ‡¶ó‡ßá‡¶≤‡ßá ‡¶∏‡¶æ‡¶á‡¶≤‡ßá‡¶®‡ßç‡¶ü‡¶≤‡¶ø ‡¶´‡ßá‡¶á‡¶≤ ‡¶π‡¶¨‡ßá

    # ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤‡¶ø‡¶Ç
    if prompt := st.chat_input("Ask a question about your documents..."):
        if not st.session_state.api_key_configured:
            st.info("Please configure your Google API Key in the sidebar first.")
        elif not st.session_state.documents_processed:
            st.info("Please upload and process your documents first.") 
        else:
            # ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
                
            # ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶ü ‡¶∞‡ßá‡¶∏‡¶™‡¶®‡ßç‡¶∏ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü
            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    genai_model = genai.GenerativeModel('gemini-pro')
                    response_dict = get_rag_response(
                        prompt,
                        st.session_state.vector_store,
                        genai_model
                    )
                    
                    st.markdown(response_dict["answer"])
                    
                    # ‡¶∞‡ßá‡¶∏‡¶™‡¶®‡ßç‡¶∏ ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
                    assistant_message = {"role": "assistant", "content": response_dict["answer"]}
                    if "table_data" in response_dict:
                        assistant_message["table_data"] = response_dict["table_data"]
                        st.dataframe(response_dict["table_data"])
                    
                    st.session_state.messages.append(assistant_message)


if __name__ == "__main__":
    main()
